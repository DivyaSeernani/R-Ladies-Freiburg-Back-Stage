Eye-Tracking Workshop
========================================================
author: Julia and Divya
date: 02.09.2020
autosize: true



The basic idea: What does eye-tracking do?
========================================================
```{r, echo=FALSE, include=FALSE, results='hide', message=FALSE, warning=FALSE}
#library(tidyverse); library(hablar); library(kableExtra)
setwd("E:/R-Ladies/R-Ladies-Freiburg-Back-Stage/Eye-tracking workshop")
```

Eye-tracking relies on the **eye-mind hypothesis/assumption**:  
What you look at is what you think about

And furthermore:  
Time is related to processing effort  
Delays hint at processing difficulties

Using eye-tracking, we can track participants' subconscious processes.  
It's an **on-line** (as opposed to off-line) measure of **processing difficulty** with a high temporal resolution.


The basic idea: What does eye-tracking do?
========================================================
type: prompt
incremental: true

Complete this sentence:  
*The old man...*

Now read this sentence:  
*The old man the boat.*

What happened when you read this sentence, and why?  

Which word(s) did you get stuck on?

Typically, people interpret *old* as an adjective which refers to *man* as a noun. But here, *old* is a noun and *man* a verb instead, so in **garden-path sentences** like these, readers need to update their interpretations.  
In this example, we'd typically see a delay on *the* because that's when readers realise their initial interpretation can't be correct.


How do eye-trackers work?
========================================================

They use a **camera** (left) and an **infrared light** (right)

```{r, echo=FALSE, fig.cap="Camera and infrared light"}
knitr::include_graphics("files/Camera.jpg")
```

***

...to track the **pupil** (green) and the **corneal reflection** (red)  
  
```{r, echo=FALSE, out.width="90%", out.height="90%", fig.cap="Pupil (green) and corneal reflection (red)"}
knitr::include_graphics("files/Pupil&cornealReflection.JPG")
```


How do eye-trackers work?
========================================================
The difference between the **center of the pupil** and the **corneal reflection** is used to calculate the position of the gaze.
***
```{r, echo=FALSE, fig.cap="Gaze calculations based on pupil center and reflections"}
knitr::include_graphics("files/GazeCalculations.JPG")
```


Eye-tracking systems
========================================================

**Three types of systems:**

- workstation/desktop
- mobile solutions
- eye-tracking glasses

***

```{r, echo=FALSE, fig.cap="Gaze calculations based on pupil center and reflections"}
knitr::include_graphics("files/Setups.jpg")
```


Eye-tracking systems: Workstation/desktop
========================================================

```{r, echo=FALSE, out.width="50%", out.height="50%", fig.cap="Desktop Trackers"}
knitr::include_graphics("files/Lab set-up.png")
```


Eye-tracking systems: Workstation/desktop
========================================================

```{r, echo=FALSE, fig.cap="Desktop Trackers"}
knitr::include_graphics("files/Workstation.jpg")
```


Eye-tracking systems: Glasses
========================================================

```{r, echo=FALSE, out.width="75%", out.height="75%",fig.cap="Mobile Trackers"}
knitr::include_graphics("files/IMG-20151230-WA0007.jpg")
```


Eye-tracking systems: Considerations
========================================================

**Practicality**
- desktop setups: participants have to travel to the lab
- mobile setups: can be done anywhere

**Naturalness**
- reading in a desktop setups is very different from how you would usually read
- mobile setups feel more natural

***

**Sampling rate**  
- measured in Hz (times per second)  
- = how many pictures the camera takes per second  
- from 25-30 Hz to 1000 Hz or more, depending on the device
- for reading experiments: at least 1000 Hz recommended
- desktop setups tend to have the highest sampling rate


Where can you use eye-tracking?
========================================================
**It's very versatile!**

- Reading (music), also in literary studies
- Cognitive tasks
- Free viewing, scene perception, visual search
- Communication between users and computer systems

***

**Applicability in**
- Linguistics
- Clinical psychology
- Consumer behaviour
- Expertise and training

...and for **many different kinds of participants**


Eye-tracking: Pros and cons
========================================================

**Advantages**
- High temporal resolution
- Non-invasive
- On-line measure of processing
- (Fairly) natural
- No other task needed

***

**Diasadvantages**
- Integrating with behavioural data
- Subjectivity of AOIs
- Operational definitions of fixations/saccades


Considerations: Experimental setup 
========================================================

- Calibration and validation [(video)](https://youtu.be/FPkRxzSE0bk?t=52)
- 'Trial'
- Checks:
  - question & feedback
  - drift correct/fixation between trials  

...while managing a long list of potential problems (contact lenses, glasses, mascara or long lashes, small eyes, wet eyes, downward-facing lashes, sunlight, electromagnetic noise in the lab...)


Typical eye-tracking procedure
========================================================

**Experimenter's view:**

```{r, echo=FALSE, fig.cap="Eyelink Interface"}
knitr::include_graphics("files/EyelinkInterface.jpg")
```

...and an example of how our eyes move while we read [(video)](https://www.youtube.com/watch?v=j8-VYcYkgqY)



Task Examples
========================================================
**An example of a reading experiment investigating gender stereotypes**

Congruent condition:  
The *doctor* enjoyed *his* day off.  
The *nurse* liked *her* new shoes.  

Incongruent condition:  
The *doctor* enjoyed *her* day off.  
The *nurse* liked *his* new shoes.  

Here, we're interested in how people behave when they get to the pronouns *his* and *her*, so the areas/regions of interest (AOIs/IAs/ROIs) are those pronouns.  
Often, the AOIs are determined by the design. There are, however, also procedures to find AOIs.


Task Examples
========================================================
**The first free viewing experiment**

```{r, echo=FALSE, out.width="75%", out.height="75%",fig.cap="Free Viewing"}
knitr::include_graphics("files/FreeViewing.jpg")
```


Task Examples
========================================================
**Embedded Figures Task**

```{r, echo=FALSE, out.width="75%", out.height="75%",fig.cap="Embedded Figures"}
knitr::include_graphics("files/EmbeddedFIgures2.jpg")
```


Task Examples
========================================================
**Embedded Figures Task**

```{r, echo=FALSE, out.width="75%", out.height="75%",fig.cap="Embedded Figures"}
knitr::include_graphics("files/EmbeddedFigures1.jpg")
```


Let's look at some scan-paths
========================================================


![A scan-path for visual search with 3 Participants](files/Scan Path_B006,C001,D003.png)


What kind of data can you get from eye-trackers?
========================================================
type: prompt
incremental: true

- Fixations
- Saccades
- Microsaccades
- Regressions
- Pupil dilation
- Blinks  

What parameters could we get from these? How could they compliment each other?

Word of caution: These are not as categorical as may seem.


What does eye-tracking data look like?
========================================================

A distinction between 'Raw' and 'Event' data...

For the ease of data processing today, we will only use simulations of fixation data.


What does eye-tracking data look like?
========================================================

After exporting the event data, you'll typically be left with a lot of variables. For example, here are the column names of an exported data file from an EyeLink 1000 eye-tracker:
```{r, echo = FALSE}
example_names <- c("RECORDING_SESSION_LABEL", "TRIAL_INDEX", "DATA_FILE", "EYE_USED", "IA_AREA", "IA_AVERAGE_FIX_PUPIL_SIZE", "IA_BOTTOM", "IA_DWELL_TIME", "IA_DWELL_TIME", "IA_DYNAMIC", "IA_END_TIME", "IA_FIRST_FIXATION_DURATION", "IA_FIRST_FIXATION_INDEX", "IA_FIRST_FIXATION_RUN_INDEX", "IA_FIRST_FIXATION_TIME", "IA_FIRST_FIXATION_VISITED_IA_COUNT", "IA_FIRST_FIXATION_X", "IA_FIRST_FIXATION_Y", "IA_FIRST_FIX_PROGRESSIVE", "IA_FIRST_RUN_DWELL_TIME", "IA_FIRST_RUN_END_TIME", "IA_FIRST_RUN_FIXATION_COUNT", "IA_FIRST_RUN_START_TIME", "IA_FIRST_SACCADE_AMPLITUDE", "IA_FIRST_SACCADE_ANGLE", "IA_FIRST_SACCADE_END_TIME", "IA_FIRST_SACCADE_INDEX", "IA_FIRST_SACCADE_START_TIME", "IA_FIXATION_COUNT", "IA_FSA_COUNT_1", "IA_FSA_COUNT_2", "IA_FSA_COUNT_3", "IA_FSA_COUNT_4", "IA_FSA_COUNT_5", "IA_FSA_COUNT_6", "IA_FSA_COUNT_7", "IA_FSA_COUNT_8", "IA_FSA_COUNT_9", "IA_FSA_COUNT_10", "IA_FSA_COUNT_11", "IA_FSA_COUNT_12", "IA_FSA_COUNT_13", "IA_FSA_COUNT_14", "IA_FSA_COUNT_15", "IA_FSA_COUNT_16", "IA_FSA_COUNT_17", "IA_FSA_COUNT_18", "IA_FSA_COUNT_19", "IA_FSA_COUNT_20", "IA_FSA_COUNT_21", "IA_FSA_COUNT_22", "IA_FSA_COUNT_23", "IA_FSA_COUNT_24", "A_FSA_COUNT_25", "IA_FSA_COUNT_26", "IA_FSA_COUNT_27", "IA_FSA_COUNT_28", "IA_FSA_COUNT_29", "IA_FSA_COUNT_30", "IA_FSA_COUNT_31", "IA_FSA_COUNT_32", "IA_FSA_COUNT_33", "IA_FSA_COUNT_34", "IA_FSA_COUNT_36", "IA_FSA_COUNT_37", "IA_FSA_COUNT_38", "IA_FSA_COUNT_39", "IA_FSA_DURATION_1", "IA_FSA_DURATION_2", "IA_FSA_DURATION_3", "IA_FSA_DURATION_4", "IA_FSA_DURATION_5", "IA_FSA_DURATION_6", "IA_FSA_DURATION_7", "IA_FSA_DURATION_8", "IA_FSA_DURATION_9", "IA_FSA_DURATION_10", "IA_FSA_DURATION_11", "IA_FSA_DURATION_12", "IA_FSA_DURATION_13", "IA_FSA_DURATION_14", "IA_FSA_DURATION_15", "IA_FSA_DURATION_16", "IA_FSA_DURATION_17", "IA_FSA_DURATION_18", "IA_FSA_DURATION_19", "IA_FSA_DURATION_20", "IA_FSA_DURATION_21", "IA_FSA_DURATION_22", "IA_FSA_DURATION_23", "IA_FSA_DURATION_24", "IA_FSA_DURATION_25", "IA_FSA_DURATION_26", "IA_FSA_DURATION_27", "IA_FSA_DURATION_28", "IA_FSA_DURATION_29", "IA_FSA_DURATION_30", "IA_FSA_DURATION_31", "IA_FSA_DURATION_32", "IA_FSA_DURATION_33", "IA_FSA_DURATION_34", "IA_FSA_DURATION_35", "IA_FSA_DURATION_36", "IA_FSA_DURATION_37", "IA_FSA_DURATION_38", "IA_FSA_DURATION_39", "IA_GROUP", "IA_ID", "IA_INSTANCES_COUNT", "IA_LABEL", "IA_LAST_FIXATION_DURATION", "IA_LAST_FIXATION_RUN", "IA_LAST_FIXATION_TIME", "IA_LAST_FIXATION_X", "IA_LAST_FIXATION_Y", "IA_LAST_RUN_DWELL_TIME", "IA_LAST_RUN_END_TIME", "IA_LAST_RUN_FIXATION", "IA_LAST_RUN_FIXATION_COUNT", "IA_LAST_RUN_START_TIME", "IA_LAST_SACCADE_AMPLITUDE", "IA_LAST_SACCADE_ANGLE", "IA_LAST_SACCADE_END_TIME", "IA_LAST_SACCADE_INDEX", "IA_LAST_SACCADE_START_TIME", "IA_LEFT", "IA_LEGAL", "IA_LEGAL_IMMEDIATE", "IA_MAX_FIX_PUPIL_SIZE", "IA_MIN_FIX_PUPIL_SIZE", "IA_POINTS", "IA_REGRESSION_IN", "IA_REGRESSION_IN_COUNT", "IA_REGRESSION_OUT", "IA_REGRESSION_OUT_COUNT", "IA_REGRESSION_OUT_FULL", "IA_REGRESSION_OUT_FULL_COUNT", "IA_REGRESSION_PATH_DURATION", "IA_RIGHT", "IA_RUN_COUNT", "IA_SECOND_FIXATION_DURATION", "IA_SECOND_FIXATION_RUN", "IA_SECOND_FIXATION_TIME", "IA_SECOND_FIXATION_X", "IA_SECOND_FIXATION_Y", "IA_SECOND_RUN_DWELL_TIME", "IA_SECOND_RUN_END_TIME", "IA_SECOND_RUN_FIXATION", "IA_SECOND_RUN_FIXATION_COUNT", "IA_SECOND_RUN_START_TIME", "IA_SELECTIVE_REGRESSION_PATH_DURATION", "IA_SKIP", "IA_SPILLOVER", "IA_START_TIME", "IA_THIRD_FIXATION_DURATION", "IA_THIRD_FIXATION_RUN", "IA_THIRD_FIXATION_TIME", "IA_THIRD_FIXATION_X", "IA_THIRD_FIXATION_Y", "IA_THIRD_RUN_DWELL_TIME", "IA_THIRD_RUN_END_TIME", "IA_THIRD_RUN_FIXATION", "IA_THIRD_RUN_FIXATION_COUNT", "IA_THIRD_RUN_START_TIME", "IA_TOP", "IA_TYPE", "IP_END_EVENT_MATCHED", "IP_END_TIME", "IP_INDEX", "IP_LABEL", "IP_START_EVENT_MATCHED", "IP_START_TIME", "REPORTING_METHOD", "TRIAL_DWELL_TIME", "TRIAL_FIXATION_COUNT", "TRIAL_IA_COUNT", "TRIAL_LABEL", "TRIAL_START_TIME", "TRIAL_TOTAL_VISITED_IA_COUNT", "ANSWER", "CORRECTNESS", "RT", "RT_START", "code", "condition", "correct_answer", "ending", "experiment", "item_id", "list", "question", "stimulus")

kbl(example_names) %>%
  kable_paper() %>%
  scroll_box(width = "100%", height = "500px")
```


What does eye-tracking data look like?
========================================================

**Reading data**

Here is some simulated data from a reading experiment like the one on gender stereotypes explained on one of the previous slides:

```{r echo = FALSE}
#simulating data
reading_data <- tibble(
  RECORDING_SESSION_LABEL = rep(c("p1", "p2", "p3", "p4", "p5", "p6", "p7", "p8", 
                                  "p9", "p10", "p11", "p12", "p13", "p14", "p15", "p16"), 
                                each = 12),
  TRIAL_INDEX = rep(rep(c("1", "2"), 
                        each = 6), 16),
  DATA_FILE = rep(c("p1.edf", "p2.edf", "p3.edf", "p4.edf", "p5.edf", "p6.edf", "p7.edf", "p8.edf", 
                                  "p9.edf", "p10.edf", "p11.edf", "p12.edf", "p13.edf", "p14.edf", "p15.edf", "p16.edf"), 
                  each = 12),
  EYE_USED = rep("RIGHT", 192),
  stimulus = rep(rep(c("The doctor enjoyed his day off.", 
                       "The nurse liked his new shoes.", 
                       "The doctor enjoyed her day off.", 
                       "The nurse liked her new shoes."), 
                     each = 6), 8),
  condition = rep(rep(c("congruent", "incongruent", "incongruent", "congruent"), 
                      each = 6), 8),
  IA_LABEL = rep(c("the", "doctor", "enjoyed", "his", "day", "off", 
                   "the", "nurse", "liked", "his", "new", "shoes", 
                   "the", "doctor", "enjoyed", "her", "day", "off", 
                   "the", "nurse", "liked", "her", "new", "shoes"), 
                 8),
  IA_FIRST_FIXATION_DURATION = rep(c(rnorm(6, 200, 20), rnorm(6, 250, 30),
                                     rnorm(6, 250, 25), rnorm(6, 200, 15)), 8),
  TRIAL_FIXATION_COUNT = rep(c("9", "12", "14", "8", "10", "15", "17", "11"), 
                             each = 24),
  IA_REGRESSION_PATH_DURATION = rep(c(rnorm(6, 300, 50), rnorm(6, 400, 80),
                                     rnorm(6, 500, 70), rnorm(6, 200, 40)), 8)
  )

socio_data <- tibble(
  participant = c("p1", "p2", "p3", "p4", "p5", "p6", "p7", "p8", 
                  "p9", "p10", "p11", "p12", "p13", "p14", "p15", "p16"),
  gender = rep(c("m", "f"), each = 8),
  age = as.integer(rnorm(16, 30, 5))
)

# exporting
#write_csv(reading_data, "data/reading_example_raw.csv")
#write_csv(socio_data, "data/sociodemographic_info.csv")

# data changed manually to add NAs and outliers
reading_data <- read_csv("data/reading_example.csv")

kbl(head(reading_data, 15)) %>%
  kable_paper() %>%
  scroll_box(width = "100%", height = "400px")
```


What does eye-tracking data look like?
========================================================

**Visual Search data**

Here is another simulated dataset. Let's spend some time talking about how this one is different.

```{r}
VisualSearch_Sim <- read_csv("data/VIsualSearch_Sim.csv")
```


Working with eye-tracking data
========================================================

Pick one of the two datasets to work on.

- Read it in
- Prepare the data for visualisation (and statistical modeling)
- Come up with a visualisation (or two)
- Optional: Run a model on the data


Visualizations in eye-tracking
========================================================

Many, many ways to do it!
- Maybe an advanced workshop on this?

Considerations
- What kind of analysis you're doing
- Which packages/softwares/shiny apps are available
- Visualizing data versus visualizing statistics



Statistical analysis of eye-tracking data
========================================================
- Scan-path analysis
- Modeling
- Mixed effects
- Between group comparisons



Working with eye-tracking data
========================================================
type: prompt
incremental: true

**Reading data**

To prepare this data for visualisation and modeling, we could
- use 'select' to remove columns we don't need (e.g. EYE_USED, DATA_FILE)
- use 'filter' to
  - narrow down the data to the IAs we're curious about (*his* and *her*)
  - remove outliers
  - remove data points if the participants answered a question incorrectly
- convert to the correct data types
- use 'join' to add additional information such as word frequency or sociodemographic characteristics of the participants


Working with eye-tracking data
========================================================
type: prompt
incremental: true

**Visual Search**

- Explore the Data 
  - some pre-processing has already been done before simulating the data.
  Can you find out what that is?
- Find first fixation on Target word, after stimulus onset
- Find mean duration in Grid
- Are there any outliers? Remove them
- Find difference between first fixation on target/grid and end of trial
- Visualise the variables you created above




Working with eye-tracking data
========================================================

```{r}
reading_data <- read_csv("data/reading_example.csv", na = ".")
reading_reduced <- reading_data %>% 
  select(-c(EYE_USED, DATA_FILE)) %>% 
  filter(IA_LABEL %in% c("his", "her"))
```

```{r, echo = FALSE}
kbl(head(reading_reduced, 15)) %>%
  kable_paper() %>%
  scroll_box(width = "100%", height = "400px")
```


Visualizations in eye-tracking
========================================================

```{r, echo = FALSE}
# super basic examples, just to give you an idea
reading_reduced %>% 
  ggplot() +
  aes(condition, IA_FIRST_FIXATION_DURATION) +
  geom_violin()
```
***
```{r, echo = FALSE}
reading_reduced %>% 
  ggplot() +
  aes(condition, IA_REGRESSION_PATH_DURATION) +
  geom_violin()
```



Resources
========================================================

**Books**

- Conklin, K., Pellicer-Sanchez, A., & Carroll, G. (2018). *Eye-tracking: A guide for applied linguistics research*. Cambridge, New York, NY: Cambridge University Press.
- Klein, C., & Ettinger, U. (Eds.). (2019). *Eye Movement Research: An Introduction to Its Scientific Foundations and Applications*. Springer Nature.

***

**Links**
- https://sccn.ucsd.edu/~mgrivich/eyetracking.html
- https://imotions.com/blog/eye-tracking-work/
- https://www.researchgate.net/figure/Eye-tracking-workstation-mobile-eye-tracker-eye-tracking-glasses-Tobii-2014-SMI_fig20_311279776
- http://www.ifa.uni.wroc.pl/linguistics/?page_id=254
- https://www.mrn.org/collaborate/imaging-resources

